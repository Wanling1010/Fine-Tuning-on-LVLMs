{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip -q\n",
    "!pip install matplotlib -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets\n",
    "!pip install transformers -q -U\n",
    "!pip install -q bitsandbytes sentencepiece accelerate loralib\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install hf_transfer -q -U\n",
    "!pip install pickleshare -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows for faster downloading\n",
    "%env HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "if not os.path.isdir(\"LLaVA\"):\n",
    "    !git clone https://github.com/haotian-liu/LLaVA.git\n",
    "else:\n",
    "    print('LLaVA directory already exists. Skipping clone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can take up to 5 mins\n",
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf -q -U\n",
    "!pip install --upgrade Pillow -q\n",
    "!pip install -e \".[train]\" -q\n",
    "!pip install flash-attn --no-build-isolation -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from PIL import Image\n",
    "import transformers\n",
    "from transformers import AutoProcessor, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# supported models for inference and training \n",
    "model_path = 'liuhaotian/llava-v1.5-7b'\n",
    "\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path = model_path, \n",
    "    model_base = None, \n",
    "    model_name = model_name,\n",
    "    cache_dir='', # it will download the model to the directory we are currently at\n",
    "    use_flash_attn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to test the model's inference\n",
    "import re \n",
    "import torch \n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "from PIL import Image \n",
    "import requests \n",
    "from io import BytesIO\n",
    "\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX, \n",
    "    DEFAULT_IMAGE_TOKEN, \n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN, \n",
    "    IMAGE_PLACEHOLDER\n",
    ")\n",
    "\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import (\n",
    "    process_images, \n",
    "    tokenizer_image_token, \n",
    "    get_model_name_from_path,\n",
    ")\n",
    "\n",
    "# common function to create prompts \n",
    "def create_prompt(query, model, model_name=model_name, caption=None):\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    if IMAGE_PLACEHOLDER in query:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            query = re.sub(IMAGE_PLACEHOLDER, image_token_se, query)\n",
    "        else:\n",
    "            query = re.sub(IMAGE_PLACEHOLDER,DEFAULT_IMAGE_TOKEN, query)\n",
    "    else: \n",
    "        if model.config.mm_use_im_start_end:\n",
    "            query = image_token_se + \"\\n\" + query\n",
    "        else:\n",
    "            query = DEFAULT_IMAGE_TOKEN + \"\\n\" + query\n",
    "    conv_mode = infer_conv_mode(model_name)\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], query)\n",
    "    if caption is not None:\n",
    "        conv.append_message(conv.roles[1], caption)\n",
    "    else:\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "    return conv.get_prompt()\n",
    "\n",
    "# common function to infer conversation mode\n",
    "def infer_conv_mode(model_name):\n",
    "    if 'llama-2' in model_name.lower():\n",
    "        return 'llava_llama_2'\n",
    "    elif 'mistral' in model_name.lower():\n",
    "        return 'mistral_instruct'\n",
    "    elif 'v1.6-34b' in model_name.lower():\n",
    "        return 'chatml_direct'\n",
    "    elif 'v1' in model_name.lower():\n",
    "        return 'llava_v1'\n",
    "    elif 'mpt' in model_name.lower():\n",
    "        return 'mpt'\n",
    "    else:\n",
    "        return 'llava_v0'\n",
    "\n",
    "# common function to process images \n",
    "def process_and_prepare_images(image_files, image_processor, model, device):\n",
    "    images = [load_image(image_file) for image_file in image_files]\n",
    "    images_tensor = process_images(\n",
    "        images, \n",
    "        image_processor, \n",
    "        model.config\n",
    "    ).to(\n",
    "        device, \n",
    "        dtype=torch.float16\n",
    "    )\n",
    "    image_sizes = [image.size for image in images]\n",
    "    return images_tensor,image_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup finetuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def expand2square(pil_img, background_color):\n",
    "    width, height = pil_img.size\n",
    "    if width == height:\n",
    "        return pil_img\n",
    "    elif width > height:\n",
    "        result = Image.new(pil_img.mode, (width, width), (int(background_color[0]*255),))\n",
    "        result.paste(pil_img, (0, (width - height) // 2))\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), (int(background_color[0]*255),))\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "def load_image(image_input):\n",
    "    # check if input is a string (path or URL):\n",
    "    if isinstance(image_input, str):\n",
    "        if image_input.startswith('http') or image_input.startswith('https'):\n",
    "            response = requests.get(image_input)\n",
    "            image = Image.open(BytesIO(response.content).convert('RGB'))\n",
    "            # In the process_images function (in mm_utils.py), modify this line:\n",
    "            image = expand2square(image, (image_processor.image_mean[0],))\n",
    "        else:\n",
    "            image = Image.open(image_input).convert('RGB')\n",
    "            # In the process_images function (in mm_utils.py), modify this line:\n",
    "            image = expand2square(image, (image_processor.image_mean[0],))\n",
    "    elif isinstance(image_input, Image.Image):\n",
    "        # input is already an image object, return as it is\n",
    "        image = image_input\n",
    "        image = expand2square(image, (image_processor.image_mean[0],))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported image input type\")\n",
    "    return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(tokenizer, model, image_processor, context_len, image_file, query, model_name=model_name, sep=',', temperature=1.0, num_beams=1, max_new_tokens=512):\n",
    "    # model\n",
    "    disable_torch_init()\n",
    "\n",
    "    # create prompt using the common function\n",
    "    prompt = create_prompt(query, model, model_name)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "\n",
    "    # process images using the common function\n",
    "    if isinstance(image_file, list):\n",
    "        images_tensor, image_sizes = process_and_prepare_images(image_file, image_processor, model, model.device)\n",
    "    elif isinstance(image_file, str):\n",
    "        images_tensor, image_sizes = process_and_prepare_images([image_file], image_processor, model, model.device)\n",
    "    else:\n",
    "        images = [image_file]\n",
    "        images_tensor, image_sizes = process_and_prepare_images(images, image_processor, model, model.device)\n",
    "    # tokenize the prompt using the custom tokenizer_image_token function\n",
    "    input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "        .unsqueeze(0)\n",
    "        .to(model.device)\n",
    "    )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids, \n",
    "            images=images_tensor, \n",
    "            image_sizes=image_sizes,\n",
    "            do_sample=temperature != 1.0,\n",
    "            temperature=temperature,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True\n",
    "        )\n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=False)[0].strip()\n",
    "    print(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# image URL\n",
    "image_url = 'https://raw.githubusercontent.com/TrelisResearch/install-guides/main/knight_and_rook.jpg'\n",
    "\n",
    "try:\n",
    "    # download image\n",
    "    response = requests.get(image_url)\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "    # open it with PIL\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # display image using matplotlib\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # now you can pass the processed image to eval_model\n",
    "    eval_model(\n",
    "        tokenizer, \n",
    "        model,\n",
    "        image_processor, \n",
    "        context_len, \n",
    "        image, \n",
    "        'what do you see in this picture?'\n",
    "    )\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching the image: {e}\")\n",
    "except IOError as e:\n",
    "    print(f\"Error opening the image: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def tokenize_and_create_labels(example_batch, image_processor, tokenizer, model, model_name, device, ignore_index=-100 ):\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    image_files = example_batch['image']\n",
    "\n",
    "    images_tensor, image_sizes = process_and_prepare_images(image_files, image_processor, model, device)\n",
    "\n",
    "    query = \"What do you see in this picture?\"\n",
    "\n",
    "    # Define a mapping from int to str labels\n",
    "    label_mapping = {\n",
    "        0: \"Covid\",\n",
    "        1: \"Normal\",\n",
    "        2: \"Viral Pneumonia\"\n",
    "    }\n",
    "\n",
    "    # Convert int labels to str\n",
    "    str_labels = [label_mapping[int(label)] for label in example_batch['label']]\n",
    "\n",
    "    # Use the str labels in tokenization\n",
    "    tokenized_conversations_without_caption = [\n",
    "        tokenizer_image_token(create_prompt(query, model, model_name, None), tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "        for _ in str_labels\n",
    "    ]\n",
    "\n",
    "    tokenized_conversations_with_caption = [\n",
    "        tokenizer_image_token(create_prompt(query, model, model_name, caption), tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "        for caption in str_labels\n",
    "    ]\n",
    "    # pad the tokenized conversations to the same length\n",
    "    input_ids = pad_sequence([tcwc.squeeze(0) for tcwc in tokenized_conversations_without_caption])\n",
    "\n",
    "    # create attention mask (1 for real tokens and 0 for padding tokens)\n",
    "    attention_mask = (input_ids != pad_token_id).long().to(device)\n",
    "\n",
    "    # create labels tensor which is a copy of input_ids but with ignore_index\n",
    "    labels = torch.full_like(input_ids, fill_value=ignore_index)\n",
    "    for i, tcwc in enumerate(tokenized_conversations_without_caption):\n",
    "        # set ignore index for tokens corresponding to convo\n",
    "        input_id_without_caption = tcwc.squeeze(0)\n",
    "        labels[i, len(input_id_without_caption):] = input_ids[i, len(input_id_without_caption):]\n",
    "    \n",
    "    inputs ={\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'images': images_tensor,\n",
    "        'image_sizes': image_sizes,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# make sure to define the function outside of the lambda \n",
    "def transform_batch(batch):\n",
    "    return tokenize_and_create_labels(batch, image_processor, tokenizer, model, model_name, device, ignore_index=-100)\n",
    "\n",
    "# load and prepare dataset\n",
    "ds = load_dataset('yuighj123/covid-19-classification')\n",
    "train_ds = ds['train']\n",
    "eval_ds = ds['test']\n",
    "\n",
    "# apply transformation function to the dataset\n",
    "train_ds.set_transform(transform_batch)\n",
    "eval_ds.set_transform(transform_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below if the above does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_create_labels(example_batch, image_processor, tokenizer, model, model_name, device, ignore_index=-100):\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    image_files = example_batch['image']\n",
    "    \n",
    "    # Process images\n",
    "    images_tensor, image_sizes = process_and_prepare_images(image_files, image_processor, model, device)\n",
    "    \n",
    "    query = \"What do you see in this picture?\"\n",
    "    \n",
    "    # Tokenize the conversations without captions\n",
    "    tokenized_conversations_without_caption = [\n",
    "        tokenizer_image_token(create_prompt(query, model, model_name, caption=None), tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "        for _ in example_batch['label']\n",
    "    ]\n",
    "    \n",
    "    # Pad the tokenized conversations to the same length\n",
    "    input_ids = pad_sequence([tcwc.squeeze(0) for tcwc in tokenized_conversations_without_caption], \n",
    "                             batch_first=True, padding_value=pad_token_id)\n",
    "    \n",
    "    # Create attention mask (1 for real tokens and 0 for padding tokens)\n",
    "    attention_mask = (input_ids != pad_token_id).long()\n",
    "    \n",
    "    # Create labels tensor which is a copy of input_ids but with ignore_index\n",
    "    labels = torch.full_like(input_ids, fill_value=ignore_index)\n",
    "    for i, tcwc in enumerate(tokenized_conversations_without_caption):\n",
    "        # Set ignore index for tokens corresponding to conversation\n",
    "        input_id_without_caption = tcwc.squeeze(0)\n",
    "        labels[i, len(input_id_without_caption):] = input_ids[i, len(input_id_without_caption):]\n",
    "    \n",
    "    inputs = {\n",
    "        'input_ids': input_ids.to(device),\n",
    "        'attention_mask': attention_mask.to(device),\n",
    "        'images': images_tensor,\n",
    "        'labels': labels.to(device)\n",
    "    }\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# Make sure to define the function outside of the lambda\n",
    "def transform_batch(batch, image_processor, tokenizer, model, model_name, device, ignore_index=-100):\n",
    "    return tokenize_and_create_labels(batch, image_processor, tokenizer, model, model_name, device, ignore_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we are doing is to freeze the main model, train those smaller adapters, and then merge back onto the main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj','k_proj','v_proj',\n",
    "        # 'fc1', 'fc2' # for llama, \n",
    "        'mm_projector' # for mistral, train instead 'mm_projector'\n",
    "        'up_proj', 'down_proj', 'gate_proj' # optionally train more linear layers\n",
    "    ], \n",
    "    lora_dropout=0.05,\n",
    "    bias='none'\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# temporarily disable the transformation to access the original data\n",
    "eval_ds.reset_format()\n",
    "\n",
    "# iterate over each example in the eval dataset\n",
    "for i in range(len(eval_ds)):\n",
    "    # access the original image and caption for the current row \n",
    "    image = eval_ds[i]['image']\n",
    "    label = eval_ds[i]['label']\n",
    "\n",
    "    # display the image using matplotlib\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    eval_model(\n",
    "        tokenizer, \n",
    "        model, \n",
    "        image_processor,\n",
    "        context_len,\n",
    "        image,\n",
    "        'What do you see in this picture?'\n",
    "    )\n",
    "    print(f\"\\nCorrect label: {label}\\n\\n\")\n",
    "\n",
    "# re-enable the transformation if needed \n",
    "eval_ds.set_transform(lambda batch: tokenize_and_create_labels(batch, image_processor, tokenizer, model, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(\"Batch keys:\", batch.keys())\n",
    "    \n",
    "    for key in batch:\n",
    "        print(f\"{key} shape: {batch[key].shape}\")\n",
    "    \n",
    "    if 'image' in batch:  # Note: changed from 'images' to 'image'\n",
    "        print('Images are included in the DataLoader')\n",
    "        image_tensor = batch['image'][0]  # Note: using index 0 instead of 1\n",
    "        print(f\"First Image Data Type: {image_tensor.dtype}\")\n",
    "        print(f\"First Image Shape: {image_tensor.shape}\")\n",
    "        print(f\"First Image Value range: [{image_tensor.min()}, {image_tensor.max()}]\")\n",
    "    \n",
    "    if 'label' in batch:  # Note: changed from 'labels' to 'label'\n",
    "        print(f\"Labels: {batch['label']}\")\n",
    "    \n",
    "    break  # only check the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below code if the above does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def transform_batch(batch, image_processor, tokenizer, model, model_name, device, ignore_index=-100):\n",
    "    # Combine all examples in the batch\n",
    "    combined_batch = {\n",
    "        'image': [item['image'] for item in batch],\n",
    "        'label': [item['label'] for item in batch]\n",
    "    }\n",
    "    \n",
    "    # Process the combined batch\n",
    "    return tokenize_and_create_labels(combined_batch, image_processor, tokenizer, model, model_name, device, ignore_index)\n",
    "\n",
    "# Create a partial function with the required arguments\n",
    "transform_batch_partial = partial(transform_batch, \n",
    "                                  image_processor=image_processor, \n",
    "                                  tokenizer=tokenizer, \n",
    "                                  model=model, \n",
    "                                  model_name=model_name, \n",
    "                                  device=device)\n",
    "\n",
    "# Create the DataLoader with the collate_fn\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=transform_batch_partial)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(\"Batch keys:\", batch.keys())\n",
    "    for key, value in batch.items():\n",
    "        if isinstance(value, (list, tuple, np.ndarray, torch.Tensor)):\n",
    "            print(f\"{key} type: {type(value)}, shape: {np.shape(value)}\")\n",
    "        else:\n",
    "            print(f\"{key} type: {type(value)}\")\n",
    "\n",
    "    if 'images' in batch:\n",
    "        print('Images are included in the DataLoader')\n",
    "        \n",
    "        # Safely print shapes\n",
    "        for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "            if key in batch:\n",
    "                print(f\"Batch '{key}' shape: {np.shape(batch[key])}\")\n",
    "        \n",
    "        # Safely print labels and attention mask\n",
    "        if 'labels' in batch:\n",
    "            labels = batch['labels'][0] if isinstance(batch['labels'], (list, tuple, np.ndarray, torch.Tensor)) else batch['labels']\n",
    "            if isinstance(labels, (np.ndarray, torch.Tensor)):\n",
    "                labels = labels.tolist()\n",
    "            labels_str = ['[IGNORE]' if label == -100 else str(label) for label in labels]\n",
    "            print(f\"Labels: {labels_str}\")\n",
    "        \n",
    "        if 'attention_mask' in batch:\n",
    "            attention_mask = batch['attention_mask'][0] if isinstance(batch['attention_mask'], (list, tuple, np.ndarray, torch.Tensor)) else batch['attention_mask']\n",
    "            if isinstance(attention_mask, (np.ndarray, torch.Tensor)):\n",
    "                attention_mask = attention_mask.tolist()\n",
    "            print(f\"Attention Mask: {attention_mask}\")\n",
    "        \n",
    "        # Display image information\n",
    "        image_data = batch['images'][0] if isinstance(batch['images'], (list, tuple, np.ndarray, torch.Tensor)) else batch['images']\n",
    "        print(f\"Image data type: {type(image_data)}\")\n",
    "        \n",
    "        if isinstance(image_data, (np.ndarray, torch.Tensor)):\n",
    "            print(f\"Image shape: {image_data.shape}\")\n",
    "            print(f\"Image dtype: {image_data.dtype}\")\n",
    "            print(f\"Image value range: [{np.min(image_data)}, {np.max(image_data)}]\")\n",
    "            \n",
    "            # Try to display the image\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            \n",
    "            if len(image_data.shape) == 3:\n",
    "                if image_data.shape[0] == 3:\n",
    "                    image_data = np.transpose(image_data, (1, 2, 0))\n",
    "                plt.imshow(image_data)\n",
    "            elif len(image_data.shape) == 2:\n",
    "                plt.imshow(image_data, cmap='gray')\n",
    "            else:\n",
    "                print(f\"Unexpected image shape: {image_data.shape}\")\n",
    "            \n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Image data is not a numpy array or torch tensor. Cannot display.\")\n",
    "    \n",
    "    break  # only check the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_name = f\"{model_name}-covid\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_model_name,\n",
    "    learning_rate=1e-4,\n",
    "    # fp16=True, #for non ampere gpus\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=0.2,\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    num_train_epochs=3,\n",
    "    # max_steps=3,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    optim=\"adamw_torch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    # compute_loss=compute_loss,  # Pass the custom compute_loss function\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Temporarily disable the transformation to access the original data\n",
    "eval_ds.reset_format()\n",
    "\n",
    "# Iterate over each example in the evaluation dataset\n",
    "for i in range(len(eval_ds)):\n",
    "    # Access the original image and caption for the current row\n",
    "    image = eval_ds[i]['image']\n",
    "    caption = eval_ds[i]['caption']\n",
    "\n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "    plt.show()\n",
    "\n",
    "eval_model(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    image_processor,\n",
    "    context_len,\n",
    "    image,\n",
    "    \"What do you see in this picture?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
